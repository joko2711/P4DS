{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb2f812",
   "metadata": {},
   "source": [
    "# Project invoices\n",
    "*KUBIK Aleksander - KOBANA Johan - JOUYIT Matthieu - Thomas BOULAINE - DIA4*\n",
    "\n",
    "\n",
    "Our problem : How can we analyze and visualize an online store’s activity using an invoice dataset to extract key indicators that support data-driven decisions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ee73779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "from prophet import Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adfa19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Function for reading a CSV file\n",
    "    Input: the path to the CSV file (string)    \n",
    "    Output: a dataframe containing the loaded dataset\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)   # Read the CSV into a pandas DataFrame\n",
    "    return df                     # Return the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13605abd",
   "metadata": {},
   "source": [
    "### **load_data()** - Load CSV Files\n",
    "**Purpose**: Read CSV files into pandas DataFrames.  \n",
    "**Input**: File path (string)  \n",
    "**Output**: DataFrame ready for analysis  \n",
    "**Use case**: Initial data loading from `invoices.csv` and city reference file  \n",
    "\n",
    "**Code Approach**:  \n",
    "Uses `pd.read_csv()` to load a CSV file directly into memory. Simple wrapper function for reusability and consistency across all data imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "776e5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(data):\n",
    "    \"\"\"\n",
    "    Function to display basic information about a dataframe\n",
    "    Input: a dataframe\n",
    "    Output: printed information (shape, columns, dtypes, missing values)\n",
    "    \"\"\"\n",
    "    print(\"Shape:\", data.shape)                     # Print number of rows and columns\n",
    "    print(\"\\nColumns:\", data.columns.tolist())      # Print list of column names\n",
    "\n",
    "    print(\"\\nData types:\")                          # Print data types of each column\n",
    "    print(data.dtypes)\n",
    "\n",
    "    print(\"\\nMissing values per column:\")           # Print missing values for each column\n",
    "    print(data.isna().sum())\n",
    "\n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    categorical_cols = [\n",
    "    'first_name', 'city', 'job'\n",
    "    ]\n",
    "    for col in categorical_cols:                    #Ranking between categorical variables\n",
    "        print(f\"\\nTop 5 values for {col}:\")         #Clients plus présents, villes dominantes\n",
    "        print(data[col].value_counts().head(5))     #professions majoritaires\n",
    "\n",
    "    numeric_ranking = data[['qty', 'amount']].agg(['mean', 'sum', 'std']).T     #Ranking between numerical variables\n",
    "    numeric_ranking = numeric_ranking.sort_values(by='sum', ascending=False)    #Most influent variable economicaly and volume\n",
    "    print(numeric_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48730264",
   "metadata": {},
   "source": [
    "### **basic_info()** - Exploratory Data Analysis\n",
    "**Purpose**: Understand the dataset structure and content.  \n",
    "**Input**: DataFrame  \n",
    "**Output**: Printed statistics (shape, columns, types, missing values, top categories, numeric rankings)  \n",
    "**Use case**: Initial dataset inspection to identify data quality and feature importance  \n",
    "\n",
    "**Code Approach**:  \n",
    "Combines `.shape`, `.dtypes`, `.isna().sum()` for structure inspection, then uses `.value_counts().head(5)` for categorical ranking and `.agg(['mean', 'sum', 'std'])` for numeric ranking to identify key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57232911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dates(data):\n",
    "    \"\"\"\n",
    "    Function to preprocess date-related fields\n",
    "    Input: a dataframe\n",
    "    Output: the same dataframe with parsed dates, changing type and extracted year/month\n",
    "    \"\"\"\n",
    "    data[\"invoice_date\"] = pd.to_datetime(data[\"invoice_date\"], format=\"%d/%m/%Y\")  # Convert date string to datetime\n",
    "    data[\"year\"] = data[\"invoice_date\"].dt.year                                      # Extract year\n",
    "    data[\"month\"] = data[\"invoice_date\"].dt.month                                    # Extract month\n",
    "    data[\"product_id\"] = data[\"product_id\"].astype(str)                              # Ensure product_id is string\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b865359",
   "metadata": {},
   "source": [
    "### **preprocess_dates()** - Temporal Data Transformation\n",
    "**Purpose**: Convert date strings to datetime format and extract temporal features.  \n",
    "**Input**: DataFrame with string dates  \n",
    "**Output**: DataFrame with datetime column + year/month columns  \n",
    "**Use case**: Enables time-series analysis and temporal grouping for monthly sales trends  \n",
    "\n",
    "**Code Approach**:  \n",
    "Applies `pd.to_datetime()` with format string to parse dates correctly, then uses `.dt.year` and `.dt.month` accessors to extract temporal components. Creates new columns for easy grouping later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c005510",
   "metadata": {},
   "source": [
    "Here we convert the `invoice_date` column into a real datetime format.  \n",
    "We also create two new columns: `year` and `month`.\n",
    "\n",
    "These will be useful later when we study trends in sales over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52407e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_by_month(invoice):\n",
    "    \"\"\"\n",
    "    Compute total monthly revenue based on 'amount'.\n",
    "    \"\"\"\n",
    "    monthly_sales = invoice.groupby([\"year\", \"month\"])[\"amount\"].sum().reset_index()\n",
    "    monthly_sales[\"date\"] = pd.to_datetime(\n",
    "        monthly_sales[\"year\"].astype(str) + \"-\" + monthly_sales[\"month\"].astype(str) + \"-01\"\n",
    "    )\n",
    "    return monthly_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc55da0",
   "metadata": {},
   "source": [
    "### **sales_by_month()** - Monthly Revenue Aggregation\n",
    "**Purpose**: Calculate total sales per month for trend analysis.  \n",
    "**Input**: Invoice DataFrame  \n",
    "**Output**: DataFrame with monthly totals + date column  \n",
    "**Use case**: Foundation for temporal visualization and forecasting (Indicator 3)  \n",
    "\n",
    "**Code Approach**:  \n",
    "Uses `.groupby(['year', 'month'])` to aggregate sales by time periods, then reconstructs a proper date column using `pd.to_datetime()` for cleaner visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07b2f7",
   "metadata": {},
   "source": [
    "This function calculates the total sales for each month.  \n",
    "We group the data by year and month, then sum the amounts.\n",
    "\n",
    "It gives us our first time-based indicator: how the store’s sales evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6dd0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_products(df, n=10):\n",
    "    \"\"\"\n",
    "    Function to compute the top-N best-selling products\n",
    "    Input: \n",
    "      df : dataframe containing at least 'product_id' and 'amount'\n",
    "      n  : number of products to return (default = 10)\n",
    "    Output: \n",
    "      a dataframe with the N products that generate the highest total amount\n",
    "    \"\"\"\n",
    "    top = (\n",
    "        df.groupby(\"product_id\")[\"amount\"]      # group by product and sum revenue\n",
    "          .sum()\n",
    "          .reset_index()                        # back to a flat dataframe\n",
    "          .sort_values(by=\"amount\", ascending=False)  # highest revenue first\n",
    "          .head(n)                              # keep only top N\n",
    "    )\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9f428",
   "metadata": {},
   "source": [
    "### **top_products()** - **INDICATOR 1: Top Revenue Products**\n",
    "**Purpose**: Identify best-selling products by total revenue (GroupBy + Aggregation).  \n",
    "**Input**: DataFrame, number of top products (n=10)  \n",
    "**Output**: Top N products with highest cumulative revenue  \n",
    "**Use case**: Stock management, marketing focus, strategic planning  \n",
    "**Visualization**: Bar chart (Product ID vs Revenue)  \n",
    "\n",
    "**Code Approach**:  \n",
    "Groups by `product_id`, sums amounts, sorts descending with `ascending=False`, and uses `.head(n)` to extract top N. Method chaining makes the pipeline clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e64259d0-1143-42d1-9676-edf9917c3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_mining_by_job(df, min_support=0.01, top_n=10):\n",
    "    \"\"\"\n",
    "    Function to extract association rules by job\n",
    "    Input:  \n",
    "      df (with columns 'job', 'product_id', 'qty')\n",
    "      min_support\n",
    "      top_n\n",
    "    Output: dataframe with the strongest association rules\n",
    "    \"\"\"\n",
    "\n",
    "    basket = df.groupby(['job', 'product_id'])['qty'].sum().unstack().fillna(0)  # job–product matrix\n",
    "    basket = basket > 0                                                           # convert quantities to booleans\n",
    "\n",
    "    itemsets = fpgrowth(basket, min_support=min_support, use_colnames=True)       # frequent itemsets\n",
    "    if itemsets.empty:                                                            # if nothing is frequent\n",
    "        return pd.DataFrame({\"message\": [\"No frequent itemsets found\"]})          # return message instead\n",
    "\n",
    "    rules = association_rules(itemsets, metric=\"lift\", min_threshold=0)           # generate association rules\n",
    "    if rules.empty:                                                               # if no rules are found\n",
    "        return pd.DataFrame({\"message\": [\"No association rules found\"]})          # return message instead\n",
    "\n",
    "    rules[\"score\"] = rules[\"lift\"] * rules[\"confidence\"]                          # combined score = lift × confidence\n",
    "    rules = rules.sort_values(by=\"score\", ascending=False).head(top_n)            # keep only top_n best rules\n",
    "\n",
    "    rules[\"antecedent_txt\"] = rules[\"antecedents\"].apply(lambda x: list(x)[0])    # convert antecedent set to text\n",
    "    rules[\"consequent_txt\"] = rules[\"consequents\"].apply(lambda x: list(x)[0])    # convert consequent set to text\n",
    "\n",
    "    def get_jobs_supporting_rule(row):                                            # helper: jobs that support a rule\n",
    "        a = row[\"antecedent_txt\"]                                                # product A (antecedent)\n",
    "        b = row[\"consequent_txt\"]                                                # product B (consequent)\n",
    "\n",
    "        jobs_A = set(df[df[\"product_id\"] == a][\"job\"])                           # jobs that bought A\n",
    "        jobs_B = set(df[df[\"product_id\"] == b][\"job\"])                           # jobs that bought B\n",
    "\n",
    "        return sorted(jobs_A.intersection(jobs_B))                               # jobs that bought both A and B\n",
    "\n",
    "    rules[\"jobs_supporting_rule\"] = rules.apply(get_jobs_supporting_rule, axis=1) # add supporting jobs to each rule\n",
    "    rules[\"num_jobs\"] = rules[\"jobs_supporting_rule\"].apply(len)\n",
    "\n",
    "    return rules[[\n",
    "        \"antecedent_txt\",                                                        # product on the left side of rule\n",
    "        \"consequent_txt\",                                                        # product on the right side of rule\n",
    "        \"confidence\",                                                            # confidence of the rule\n",
    "        \"lift\",                                                                  # lift of the rule\n",
    "        \"score\",                                                                 # combined score (lift × confidence)\n",
    "        \"jobs_supporting_rule\",                                                  # list of jobs supporting the rule\n",
    "        \"num_jobs\"\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1522e68",
   "metadata": {},
   "source": [
    "### **pattern_mining_by_job()** - **INDICATOR 2: Product Association Rules**\n",
    "**Purpose**: Discover product pairs bought together using FP-Growth + Association Rules (Frequent Pattern Mining).  \n",
    "**Input**: DataFrame, min_support, top_n rules  \n",
    "**Output**: Association rules with confidence, lift, and supporting job categories  \n",
    "**Use case**: Cross-selling strategies, bundling, promotions  \n",
    "**Visualization**: Scatter plot (Confidence vs Lift, sized by score)  \n",
    "\n",
    "**Code Approach**:  \n",
    "1. Creates job-product matrix using `.groupby().unstack()` and converts to boolean\n",
    "2. Applies `fpgrowth()` to find frequent itemsets above min_support threshold\n",
    "3. Uses `association_rules()` with lift metric to generate rules\n",
    "4. Scores rules as `lift × confidence` and ranks top_n\n",
    "5. Maps job support using set intersection: jobs that bought both A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1b32ffd-aa61-4787-af55-b99f9c2a42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "def spatial_analysis_by_city(\n",
    "    df,\n",
    "    city_col='city',\n",
    "    amount_col='amount',\n",
    "    geocode=False,\n",
    "    top_n=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform spatial analysis using city information:\n",
    "    - aggregate invoice count and total revenue per city\n",
    "    - optionally geocode only the top N cities\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregation\n",
    "    city_agg = (\n",
    "        df\n",
    "        .groupby(city_col)\n",
    "        .agg(\n",
    "            invoice_count=(city_col, 'count'),\n",
    "            total_revenue=(amount_col, 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(by='invoice_count', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Keep only top N cities (important)\n",
    "    city_agg = city_agg.head(top_n)\n",
    "\n",
    "    # Optional geocoding\n",
    "    if geocode:\n",
    "        geolocator = Nominatim(user_agent=\"spatial_analysis_tp\")\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for city in city_agg[city_col]:\n",
    "            location = geolocator.geocode(f\"{city}, France\")\n",
    "            if location:\n",
    "                latitudes.append(location.latitude)\n",
    "                longitudes.append(location.longitude)\n",
    "            else:\n",
    "                latitudes.append(None)\n",
    "                longitudes.append(None)\n",
    "            time.sleep(1)  # respect Nominatim policy\n",
    "\n",
    "        city_agg['latitude'] = latitudes\n",
    "        city_agg['longitude'] = longitudes\n",
    "\n",
    "    return city_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df15eaa",
   "metadata": {},
   "source": [
    "### **spatial_analysis_by_city()** - **INDICATOR 4: Geographic Clustering**\n",
    "**Purpose**: Analyze commercial activity by city + optional geocoding.  \n",
    "**Input**: DataFrame, city column, amount column, geocode flag  \n",
    "**Output**: Top N cities with invoice counts, revenue, and coordinates  \n",
    "**Use case**: Logistics optimization, regional marketing, distribution strategy  \n",
    "**Visualization**: Mapbox scatter (geographic position, circle size = invoices, color = revenue)  \n",
    "\n",
    "**Code Approach**:  \n",
    "Aggregates data using `.groupby().agg()` with multiple metrics (count, sum), sorts by invoice count, and if `geocode=True`, uses Nominatim API to convert city names to lat/lon coordinates. Rate limiting (1 sec delay) respects API policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b73843-5c82-46f1-90df-1c609d980d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_analysis(df, last_n_years=5, window_months=6, forecast_months=0):\n",
    "    \"\"\"\n",
    "    Function for temporal analysis + optional Prophet forecasting\n",
    "    Input:  \n",
    "      df (invoice dataset)\n",
    "      last_n_years\n",
    "      window_months\n",
    "      forecast_months\n",
    "    Output: \n",
    "      monthly_df (clean monthly series)\n",
    "      forecast_df (if enabled)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()                                                                  # work on a local copy\n",
    "\n",
    "    max_year = df[\"invoice_date\"].dt.year.max()                                     # most recent year in dataset\n",
    "    min_year = max_year - last_n_years + 1                                          # oldest year to keep\n",
    "    df = df[df[\"invoice_date\"].dt.year >= min_year]                                 # filter selected years\n",
    "\n",
    "    monthly = (                                                                     # compute monthly revenue\n",
    "        df.groupby(df[\"invoice_date\"].dt.to_period(\"M\"))[\"amount\"]                  \n",
    "        .sum()\n",
    "        .to_timestamp()\n",
    "        .reset_index(name=\"amount\")\n",
    "        .rename(columns={\"invoice_date\": \"date\"})                                    # rename for clarity\n",
    "    )\n",
    "\n",
    "    monthly[\"trend\"] = monthly[\"amount\"].rolling(window=window_months).mean()        # rolling mean trend\n",
    "\n",
    "    if forecast_months > 0:                                                          # forecasting enabled?\n",
    "        prophet_df = monthly.rename(columns={\"date\": \"ds\", \"amount\": \"y\"})            # Prophet column format\n",
    "        model = Prophet(yearly_seasonality=True)                                      # Prophet model\n",
    "        model.fit(prophet_df[[\"ds\", \"y\"]])                                            # train model\n",
    "        future = model.make_future_dataframe(periods=forecast_months, freq=\"ME\")      # extend timeline\n",
    "        forecast = model.predict(future)                                              # generate forecast\n",
    "    else:\n",
    "        forecast = None                                                               # no forecasting\n",
    "\n",
    "    return monthly, forecast                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54084791",
   "metadata": {},
   "source": [
    "### **temporal_analysis()** - **INDICATOR 3: Time-Series Forecasting**\n",
    "**Purpose**: Extract temporal trends and forecast future sales using Prophet.  \n",
    "**Input**: DataFrame, time window, forecast horizon  \n",
    "**Output**: Monthly aggregated data + Prophet forecast with confidence intervals  \n",
    "**Use case**: Budget planning, inventory management, strategic forecasting  \n",
    "**Visualization**: Line chart (actual sales, trend line, forecast + confidence zone)  \n",
    "\n",
    "**Code Approach**:  \n",
    "1. Filters data to last N years using year comparison\n",
    "2. Groups by month using `.dt.to_period('M')` and aggregates revenue\n",
    "3. Calculates rolling average trend using `.rolling(window=window_months).mean()`\n",
    "4. If forecasting enabled: fits Prophet model, generates future dates, and predicts yhat + confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf5ed589-464c-4f26-b7c7-27153e8afe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline\n",
    "def main():\n",
    "\n",
    "    # --------------------------\n",
    "    # 1. Load and preprocess data\n",
    "    # --------------------------\n",
    "    invoice = load_data(\"invoices.csv\")\n",
    "    df_cities = load_data(\"CitiesofFrance-VillesdeFrance.csv\")\n",
    "\n",
    "    cities = df_cities.iloc[:, 0].dropna().unique()\n",
    "    invoice['city'] = np.random.choice(\n",
    "        cities,\n",
    "        size=len(invoice),\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    basic_info(invoice)\n",
    "    invoice = preprocess_dates(invoice)\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. Compute core indicators\n",
    "    # --------------------------\n",
    "    monthly_sales = sales_by_month(invoice)\n",
    "    top10 = top_products(invoice)\n",
    "    avg_basket = average_basket(invoice)\n",
    "\n",
    "    # --------------------------\n",
    "    # 3. Pattern mining (job-based rules)\n",
    "    # --------------------------\n",
    "    rules = pattern_mining_by_job(invoice, min_support=0.02, top_n=10)\n",
    "\n",
    "    # --------------------------\n",
    "    # 4. Temporal analysis + Prophet forecasting\n",
    "    # --------------------------\n",
    "    monthly, forecast = temporal_analysis(\n",
    "    invoice,\n",
    "    last_n_years=10,\n",
    "    window_months=6,\n",
    "    forecast_months=12\n",
    "    )\n",
    "\n",
    "    # --------------------------\n",
    "    # 5. City spatial analysis (commercial activity)\n",
    "    # --------------------------\n",
    "    city_amount = spatial_analysis_by_city(invoice,geocode=True)\n",
    "\n",
    "    print(\"Pipeline executed successfully.\")\n",
    "\n",
    "    # Return all outputs\n",
    "    return {\n",
    "        \"invoice\": invoice,\n",
    "        \"monthly_sales\": monthly_sales,\n",
    "        \"top10\": top10,\n",
    "        \"avg_basket\": avg_basket,\n",
    "        \"rules\": rules,\n",
    "        \"monthly\": monthly,\n",
    "        \"forecast\": forecast,\n",
    "        \"city_amount\" : city_amount\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a54d8c",
   "metadata": {},
   "source": [
    "### **main()** - Pipeline Orchestration\n",
    "**Purpose**: Execute the complete data analysis pipeline in sequence.  \n",
    "**Steps**:\n",
    "1. Load raw data + assign cities\n",
    "2. Explore dataset structure\n",
    "3. Extract 4 key indicators (top products, patterns, forecasts, geographic clusters)\n",
    "4. Return all results as dictionary  \n",
    "**Use case**: Central function called by Dash app to generate all visualizations  \n",
    "\n",
    "**Code Approach**:  \n",
    "Orchestrates all functions in correct order: load → preprocess → compute indicators → combine results into a dictionary. Uses `np.random.choice()` to assign random French cities (since data lacks real locations). Returns dict allows flexible access by Dash app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ed65119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000, 11)\n",
      "\n",
      "Columns: ['first_name', 'last_name', 'email', 'product_id', 'qty', 'amount', 'invoice_date', 'address', 'city', 'stock_code', 'job']\n",
      "\n",
      "Data types:\n",
      "first_name       object\n",
      "last_name        object\n",
      "email            object\n",
      "product_id        int64\n",
      "qty               int64\n",
      "amount          float64\n",
      "invoice_date     object\n",
      "address          object\n",
      "city             object\n",
      "stock_code        int64\n",
      "job              object\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "first_name      0\n",
      "last_name       0\n",
      "email           0\n",
      "product_id      0\n",
      "qty             0\n",
      "amount          0\n",
      "invoice_date    0\n",
      "address         0\n",
      "city            0\n",
      "stock_code      0\n",
      "job             0\n",
      "dtype: int64\n",
      "\n",
      "Descriptive statistics:\n",
      "\n",
      "Top 5 values for first_name:\n",
      "first_name\n",
      "David Williams        6\n",
      "Daniel Johnson        5\n",
      "Melissa Johnson       5\n",
      "Michael Brown         5\n",
      "Christopher Garcia    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 5 values for city:\n",
      "city\n",
      "Rueil-la-Gadelière    4\n",
      "Orus                  4\n",
      "Mathonville           4\n",
      "Hombourg-Budange      4\n",
      "Valbonnais            4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 5 values for job:\n",
      "job\n",
      "Research scientist (life sciences)    29\n",
      "Agricultural consultant               28\n",
      "Engineer, manufacturing               28\n",
      "Producer, radio                       27\n",
      "Heritage manager                      26\n",
      "Name: count, dtype: int64\n",
      "             mean        sum        std\n",
      "amount  52.918236  529182.36  27.434579\n",
      "qty      5.005900   50059.00   2.576767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:53:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:53:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline executed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksander\\AppData\\Local\\Temp\\ipykernel_17396\\104563914.py:87: DeprecationWarning:\n",
      "\n",
      "*scatter_mapbox* is deprecated! Use *scatter_map* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\plotly\\express\\_core.py:2530: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\_plotly_utils\\basevalidators.py:2640: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\plotly\\express\\_core.py:2557: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\_plotly_utils\\basevalidators.py:2640: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x189cb5e42d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dash import Dash, dcc, html, dash_table\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.express as px\n",
    "\n",
    "#Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data = main()\n",
    "\n",
    "# --------- Run your pipeline ---------\n",
    "top10 = data[\"top10\"]\n",
    "rules = data[\"rules\"]\n",
    "monthly = data[\"monthly\"]\n",
    "forecast = data[\"forecast\"]\n",
    "city = data[\"city_amount\"]\n",
    "\n",
    "# ============================================\n",
    "# 1. FIGURE Top 10 Products\n",
    "# ============================================\n",
    "fig_top10 = px.bar(\n",
    "    top10,\n",
    "    x=\"product_id\",\n",
    "    y=\"amount\",\n",
    "    title=\"Top 10 Products\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 2. FIGURE Pattern Mining Rules\n",
    "# ============================================\n",
    "fig_rules = px.scatter(\n",
    "    rules,\n",
    "    x=\"confidence\",\n",
    "    y=\"lift\",\n",
    "    size=\"score\",\n",
    "    color=\"consequent_txt\",\n",
    "    hover_name=\"antecedent_txt\",\n",
    "    hover_data=[\"num_jobs\"],\n",
    "    title=\"Association Rules (Products Bought Together)\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 3. FIGURE Temporal Analysis (monthly + trend + forecast)\n",
    "# ============================================\n",
    "\n",
    "fig_temp = px.line(\n",
    "    monthly,\n",
    "    x=\"date\",\n",
    "    y=\"amount\",\n",
    "    title=\"Monthly Sales (Last 10 Years)\"\n",
    ")\n",
    "fig_temp.data[0].name = \"Monthly Sales\"\n",
    "fig_temp.data[0].showlegend = True\n",
    "\n",
    "# Add trend line\n",
    "fig_temp.add_scatter(\n",
    "    x=monthly[\"date\"],\n",
    "    y=monthly[\"trend\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"Trend (Rolling Mean)\",\n",
    "    line=dict(color=\"orange\", width=3)\n",
    ")\n",
    "\n",
    "# Add forecast\n",
    "if forecast is not None:\n",
    "    fig_temp.add_scatter(\n",
    "        x=forecast[\"ds\"],\n",
    "        y=forecast[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"green\", width=2)\n",
    "    )\n",
    "    future = forecast[forecast[\"ds\"] > monthly[\"date\"].max()]\n",
    "\n",
    "    fig_temp.add_scatter(\n",
    "        x=list(future[\"ds\"]) + list(future[\"ds\"][::-1]),\n",
    "        y=list(future[\"yhat_upper\"]) + list(future[\"yhat_lower\"][::-1]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(0, 128, 0, 0.15)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "        name=\"Confidence Interval\"\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# 4. FIGURE City Clustering\n",
    "# ============================================\n",
    "\n",
    "fig_city = px.scatter_mapbox(\n",
    "    city.head(10),\n",
    "    lat='latitude',\n",
    "    lon='longitude',\n",
    "    size='invoice_count',          # taille des cercles\n",
    "    color='total_revenue',         # couleur = revenu\n",
    "    hover_name='city',\n",
    "    size_max=20,\n",
    "    zoom=5,\n",
    "    title='Spatial Distribution of Invoice Activity by City',\n",
    "    labels={\n",
    "        'invoice_count': 'Number of Invoices',\n",
    "        'total_revenue': 'Total Revenue'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig_city.update_layout(\n",
    "    mapbox_style='carto-positron',\n",
    "    margin=dict(r=0, t=40, l=0, b=0)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# DASH APP LAYOUT\n",
    "# ============================================\n",
    "\n",
    "app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = html.Div([\n",
    "    \n",
    "    html.H1(\"Invoices Dashboard\", style={\"textAlign\": \"center\"}),\n",
    "\n",
    "    html.H2(\"Top 10 Products\"),\n",
    "    dcc.Graph(figure=fig_top10),\n",
    "\n",
    "    html.H2(\"Pattern Mining (Job-Based Rules)\"),\n",
    "    dcc.Graph(figure=fig_rules),\n",
    "\n",
    "    html.H2(\"Temporal Analysis (Monthly + Trend + Forecast)\"),\n",
    "    dcc.Graph(figure=fig_temp),\n",
    "\n",
    "    html.H2(\"City Clustering (Commercial Activity)\"),\n",
    "    dcc.Graph(figure=fig_city)\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fbbd1",
   "metadata": {},
   "source": [
    "### **Dash Interactive Dashboard** - Final Presentation\n",
    "**Purpose**: Create interactive web dashboard displaying all 4 key indicators.  \n",
    "**Components**:\n",
    "1. Bar chart: Top 10 products\n",
    "2. Scatter plot: Association rules\n",
    "3. Line chart: Temporal trends + forecast\n",
    "4. Mapbox: Geographic distribution  \n",
    "**Use case**: Real-time business intelligence, stakeholder communication, decision support  \n",
    "**Run**: Execute this cell to launch the Dash server (http://127.0.0.1:8050/)  \n",
    "\n",
    "**Code Approach**:  \n",
    "Uses Plotly Express to create interactive figures, then wraps them in Dash HTML layout with Bootstrap styling. Scatter layers added dynamically (trend + forecast + CI) to temporal chart. Mapbox uses CARTO positron style for clean cartography. `app.run(debug=True)` launches local server with hot-reload enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6d5fd",
   "metadata": {},
   "source": [
    "## Explanation of 4 Key Indicators\n",
    "\n",
    "### **Top 10 Products** (Bar Chart - Blue)\n",
    "**What is it?**  \n",
    "The 10 products that generate the **highest revenue** for the store.\n",
    "\n",
    "**Concrete Interpretation:**\n",
    "- **Product 164** is the champion (~7000€) → our best-seller\n",
    "- The 10 products are relatively balanced between 6000-7000€\n",
    "- **Action**: Maintain sufficient stock on these 10 products\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern Mining - Association Rules** (Scatter Plot - Colors)\n",
    "**What is it?**  \n",
    "The **pairs of products bought together** by customers in the same profession.\n",
    "\n",
    "**Graph Axes**:\n",
    "- **Confidence (X-axis)**: Probability that if customer buys A, they also buy B (27%-31%)\n",
    "- **Lift (Y-axis)**: How many times more likely than random (1.7x to 2.0x)\n",
    "- **Point Size**: Combined rule strength (score = lift × confidence)\n",
    "- **Color**: Consequent product (product B purchased second)\n",
    "\n",
    "**Concrete Interpretation**:\n",
    "- Point in **top-right** = best rule (high confidence and lift)\n",
    "- **Example**: If a customer buys Product 189, there's a 30% chance they'll also buy Product X\n",
    "- **Action**: Propose these pairs as bundles or promotions\n",
    "\n",
    "---\n",
    "\n",
    "### **Temporal Analysis** (Line Chart - Blue/Orange/Green)\n",
    "**What is it?**  \n",
    "The **sales evolution** over 10 years + future predictions.\n",
    "\n",
    "**3 Curves**:\n",
    "1. **Blue** (Monthly Sales): Actual monthly sales (noisy, volatile)\n",
    "2. **Orange** (Trend): Smooth trend (6-month rolling average) → stable general direction\n",
    "3. **Green** (Forecast): Prophet predictions for next 12 months\n",
    "4. **Green Zone**: Confidence interval (95%) → uncertainty increases with time\n",
    "\n",
    "**Concrete Interpretation**:\n",
    "- Stable trend → no major growth/decline\n",
    "- Confidence zone widens towards future (uncertainty increases)\n",
    "- **Action**: Plan 12 months of inventory based on Trend\n",
    "\n",
    "---\n",
    "\n",
    "### **City Clustering - Spatial Distribution** (Map)\n",
    "**What is it?**  \n",
    "The **location of cities** based on their commercial activity.\n",
    "\n",
    "**Map Elements**:\n",
    "- **Position**: True geographic location (latitude/longitude)\n",
    "- **Circle Size**: Number of invoices per city (larger = more purchases)\n",
    "- **Color**: Total revenue generated (yellow = richer, blue = less)\n",
    "\n",
    "**Concrete Interpretation**:\n",
    "- **Large yellow circles**: VIP cities (many purchases + high amounts)\n",
    "- **Blue circles**: Emerging or niche cities\n",
    "- **Action**: Invest in logistics for major cities (Paris, Lyon)\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary - What decisions should we take?**\n",
    "\n",
    "| Indicator | Commercial Decision |\n",
    "|-----------|---------------------|\n",
    "| **Top 10 products** | Stock management, marketing campaigns |\n",
    "| **Association Rules** | Bundles, cross-selling strategies |\n",
    "| **Temporal Analysis** | Budget planning, annual forecasts |\n",
    "| **City Clustering** | Distribution, logistics infrastructure |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
